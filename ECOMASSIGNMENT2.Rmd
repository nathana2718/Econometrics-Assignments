---
title: "ECOMAss2"
author: "NathanAung"
date: "April 18, 2017"
output: pdf_document
---

```{r setup, include=TRUE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
library(R6) #using R6 oop system
library(AER)
library(stargazer)
```

##Q1
Define two classes that generates results:
```{r}
#Class that generates all combinations in a vector
regMCloop = R6Class(
  public = list(
    nrange=NULL,
    sigmarange=NULL,
    Urange=NULL,
    a1range=NULL,
    a2range=NULL,
    results = NULL,
    
    initialize = function(nrange,sigmarange,Urange,a2range=0,a1range=1)
    {
      self$nrange = nrange
      self$sigmarange = sigmarange
      self$Urange = Urange
      self$a2range = a2range
      self$a1range = a1range
      self$results = vector("list",length(nrange)*length(sigmarange)*length(Urange)*length(a2range))#list the regMC objects in a vector
    },
    
    loop = function( a3=1, b1=1, b2=1, b3=0,IV=FALSE)
    {
      i=1#elements of result vector
      for(n in self$nrange)
      {
        for(sigma in self$sigmarange)
        {
          for(dist in self$Urange)
          {
            for(a2 in self$a2range)
            {
              for(a1 in self$a1range)
                {
                  self$results[[i]] = regMClass$new(ns=n,sigma=sigma,dist=dist, a2=a2, a3=a3, b1=b1, b2=b2, b3=b3, a1=a1,IV=IV)
                  i=i+1#next result
                }
            }
          }
        }
      }
    }
    
    
  )
)

```

```{r}
regMClass = R6Class(
  public = list(
    bias = NULL,
    sd = NULL,
    CovRate=NULL,
    CovLength=NULL,
    ns = NULL,
    sigma= NULL,
    dist= NULL,
    a2 =NULL,
    a1 =NULL,
    
    
    initialize = function(ns = 20, n=1000, b1=1, b2=1, b3=0, a1=1, a2=0, a3=1, sigma=1,dist="norm", IV=FALSE)
    { 
      intervalrange =vector(mode="numeric",length=0)
      b1list = vector(mode="numeric",length=0)
      inInterval=vector(mode="logical",length=0)#declare vectors so that append method works
      for(j in 1:n)
      {
        #__init__
        Z1 = rnorm(n=ns,mean=0,sd=1)
        X2 = rnorm(n=ns,mean=0,sd=1)
        X3 = rnorm(n=ns,mean=0,sd=1)
        V = rnorm(n=ns,mean=0,sd=1)
        
        if (dist == "norm")
        {
          U = rnorm(n=ns,mean=0,sd=1) 
        }
        else
        {
          U = rlnorm(n=ns,mean=0,sd=1)
        }
        #Generate X1 and Y
        X1 = a1*Z1 + a2*X2 + a3*X3 +V
        Y = b1*X1 + b2*X2 +b3*X3 + sigma*U
        #Estimates
        if(IV==FALSE){
        hat = lm(Y ~ X1+X2)
        }
        
        else{
        hat = ivreg(Y~X1+X2|X2+Z1)  
        }
        b1list=append(b1list,hat$coefficients["X1"])
        #In confidence interval?
        interval = confint(hat,parm = "X1",interval="confidence")
        inInterval = append(inInterval,b1<interval[2]&b1>interval[1])
        intervalrange = append(intervalrange,interval[2]-interval[1])
      }
      self$ns = ns # so we can sort by values used
      self$sigma = sigma
      self$dist = dist
      self$a2 = a2
      self$a1 = a1
      self$bias = mean(b1list)-1
      self$sd = sd(b1list)
      self$CovRate = mean(inInterval)
      self$CovLength = mean(intervalrange)
    }
  )
)
```


We use a nested loop to generate all combinations needed and store them in a vector of results. These results are then stored within an object. It is possible that a recursive function approach could have been more elegant but for the number of combinations here, nested loop should suffice.

```{R}
dataQ1 = regMCloop$new(nrange=c(20,200,2000),sigmarange=c(1,2),Urange=c("norm","lognorm"))
dataQ1$loop()
```
Here we create an instance of the class and use the loop methods to generate the results. This does take some time(~20s) as we are looping over quite a large number of iterations.
```{r results='asis',echo=FALSE}
    Bias = vector(length=12)
    SD = vector(length=12)
    CovRate = vector(length=12)
    CovLength = vector(length=12)
    for(i in 1:12){
    Bias[i] = (dataQ1$results[[i]]$bias)
    SD[i] =   (dataQ1$results[[i]]$sd)
    CovRate[i] = (dataQ1$results[[i]]$CovRate)
    CovLength[i]=(dataQ1$results[[i]]$CovLength)
    }
    ndf=c(rep(20,4),rep(200,4),rep(2000,4))
    sigma  = c(rep(1,2),rep(2,2),rep(1,2),rep(2,2),rep(1,2),rep(2,2))
    Distribution = rep(c("Normal","LogNormal"),6)
    Q1=data.frame(ndf,sigma,Distribution,Bias,SD,CovRate,CovLength)
    stargazer(Q1,header=FALSE,type="latex",summary=FALSE)
```

#a)
In all combinations, $|\textrm{bias}|$ is unanimously within $\pm$ $0.005$ of 0. Considering that even under all combinations in which the number of samples is 20 that this is present, these results strongly indicate that the estimator is or very close to unbiased. It is impossible to say definitively due to the stochastic nature of the simulation, however the weak law of large numbers and central limit theorem point to this conclusion. This seems appropriate considering the theoretical aspects of the estimator. (more explanation needed here?)

#b)
For each combination, of $\sigma$ and distribution, we vary the number of samples and observe the results. In nearly all cases, bias decreases monotonically throughout.  In the few cases where it does not, bias is very close to zero and is still within the same order of magnitude of the previous estimate. This is to be expected even with a consistent estimator as the estimator becomes closer and closer to the true value of the variable. Perhaps with a higher number of simulations, this error would be reduced. Further, testing across a larger number of sample sizes may have graphically indicated the monotonicity of the bias as sample size increased. With only three data points, there could be any number of small changes in between. 

#c)
Coverage rate is consistently within 0.15 of the expected $95\%$ coverage. Coverage rates between normal and log normal distributions appear to be inconclusive. Similarly, $\sigma$ appears to have limited effect on coverage rate.  The number of samples also inconclusive. One could of course perform an anova test to show these and reveal possible interaction effects.

#d)
For all combinations, log normal generates much large confidence intervals than its normal counterpart.

This can be seen by examining the variance of normal and log normal distributions. With standard deviation $\sigma$, the variance for normal and lognormal respectively is:

$$ \sigma ^2 $$

$$ (e^{\sigma^2} -1)(e^{2\mu+\sigma^2})$$

One can verify that for all values the variance will be greater for the log normal case . Therefore the estimated standard deviation will be much larger for the distribution and thus parameter estimates will have a larger confidence interval for the same coverage rate.

This can be verified again as for unitary $\sigma$, the coverage rate is far smaller than their $\sigma=2$ counterparts. 

Sample size reduces interval length drastically for all combinations. 
2a)
```{r}
dataQ2a = regMCloop$new(nrange=c(20,200,2000),sigmarange=c(1,2),Urange=c("norm","lognorm"))
dataQ2a$loop(b3=1,a3=1)
```
```{r results='asis',echo=FALSE}
Bias = vector(length=12)
    SD = vector(length=12)
    CovRate = vector(length=12)
    CovLength = vector(length=12)
    for(i in 1:12){
    Bias[i] = (dataQ2a$results[[i]]$bias)
    SD[i] =   (dataQ2a$results[[i]]$sd)
    CovRate[i] = (dataQ2a$results[[i]]$CovRate)
    CovLength[i]=(dataQ2a$results[[i]]$CovLength)
    }
    ndf=c(rep(20,4),rep(200,4),rep(2000,4))
    sigma  = c(rep(1,2),rep(2,2),rep(1,2),rep(2,2),rep(1,2),rep(2,2))
    Distribution = rep(c("Normal","LogNormal"),6)
    Q2a=data.frame(ndf,sigma,Distribution,Bias,SD,CovRate,CovLength)
    stargazer(Q2a,header=FALSE,type="latex",summary=FALSE)
```
Calculate bias here:
It is clear that omitted variable bias is present here with an apparent consistent bias of ~0.33.
We can calculate the theoretical bias here:
$$y = X_1\beta_1 + X_2 \beta_2 +X_3\beta_3 +U $$

$$X_1 = \alpha_1Z_1+\alpha_3 X_3 + V_i$$
OLS estimates:
$$E(\hat{\beta_1}) = E\frac{\textrm{Cov}(X_1,y)}{\textrm{Var}(X_1)}$$ 
$$E(\hat{\beta_1}) = E \frac{\textrm{Cov}(X_1,X_1\beta_1 + X_2 \beta_2 +X_3\beta_3 +U)}{\textrm{Var}(Z_1 + X_3 +U)}$$
$$E(\hat{\beta_1}) = E \frac{\textrm{Cov}(X_1,X_1\beta_1) + \textrm{Cov}(X_1,X_2 \beta_2) + \textrm{Cov}(X_1,X_3\beta_3) + \textrm{Cov}(X_1,U)}{\textrm{Var}(Z_1 + X_3 +U)} $$
$$E(\hat{\beta_1}) = E \frac{\beta_1Var(X_1) +  \beta_20 + \textrm{Cov}(Z_1+X_3+U,X_3\beta_3) + 0}{\textrm{Var}(Z_1 + X_3 +U)} $$
$$E(\hat{\beta_1}) = E(\beta_1 + \frac{ \textrm{Cov}(Z_1+X_3+U,X_3\beta_3)}{\textrm{Var}(Z_1 + X_3 +U)}) $$
$$E(\hat{\beta_1}) = E (\beta_1 + \frac{\beta_3 \textrm{Cov}(Z_1,X_3) + \beta_3 \textrm{Cov}(X_3,X_3)+\textrm{Cov}(U,X_3) }{1 + 1 +1} ) $$
$$E(\hat{\beta_1}) = \beta_1 + \frac{  1 }{3} $$
$$\textrm{Bias} = \frac{1}{3}$$
Standard deviation is also uniformly higher compared to a) and as such confidence intervals are larger. Coverage rate is especially poor here indicating a severe deviation from the requirements and assumptions of linear regression. 

Whilst $X_3$ determines $X_1$ in both 1) and 2a), it does not determine $Y$ in 1). As such, it does not fulfill  the requirements for an omitted variable bias as endogeniety does not occur:  

1) The omitted variable must be a determinant of the dependant variable(ie. regression coefficient is non-zero)
2) Omitted variable is correlated with the explanatory variable. 

Whilst condition 2) is present through both questions, condition 1 only holds for question 2a).


2b)
```{r}
dataQ2b = regMCloop$new(nrange=c(20,200,2000),sigmarange=c(1,2),Urange=c("norm","lognorm"))
dataQ2b$loop(b3=1,a3=0)
```

```{r results='asis',echo=FALSE}
    Bias = vector(length=12)
    SD = vector(length=12)
    CovRate = vector(length=12)
    CovLength = vector(length=12)
    for(i in 1:12){
    Bias[i] = (dataQ2b$results[[i]]$bias)
    SD[i] =   (dataQ2b$results[[i]]$sd)
    CovRate[i] = (dataQ2b$results[[i]]$CovRate)
    CovLength[i]=(dataQ2b$results[[i]]$CovLength)
    }
    ndf=c(rep(20,4),rep(200,4),rep(2000,4))
    sigma  = c(rep(1,2),rep(2,2),rep(1,2),rep(2,2),rep(1,2),rep(2,2))
    Distribution = rep(c("Normal","LogNormal"),6)
    Q2b=data.frame(ndf,sigma,Distribution,Bias,SD,CovRate,CovLength)
    stargazer(Q2b,header=FALSE,type="latex",summary=FALSE)
```

As expected bias disappears as condition 1 is removed. The another anomalies can be explained as follows:
Insert math here

3a)
Yes, it satisfies the two conditions required for an IV.

1) Z is not correlated with Y other than through $X_1$
2) Z is correlated with $X_1$



3b)
```{r}
dataQ3 = regMCloop$new(nrange=c(20,200,2000),sigmarange=c(1,2),Urange=c("norm","lognorm"))
dataQ3$loop(b3=1,IV=TRUE)
```

```{r results='asis',echo=FALSE}
    Bias = vector(length=12)
    SD = vector(length=12)
    CovRate = vector(length=12)
    CovLength = vector(length=12)
    for(i in 1:12){
    Bias[i] = (dataQ3$results[[i]]$bias)
    SD[i] =   (dataQ3$results[[i]]$sd)
    CovRate[i] = (dataQ3$results[[i]]$CovRate)
    CovLength[i]=(dataQ3$results[[i]]$CovLength)
    }
    ndf=c(rep(20,4),rep(200,4),rep(2000,4))
    sigma  = c(rep(1,2),rep(2,2),rep(1,2),rep(2,2),rep(1,2),rep(2,2))
    Distribution = rep(c("Normal","LogNormal"),6)
    Q3=data.frame(ndf,sigma,Distribution,Bias,SD,CovRate,CovLength)
    stargazer(Q3,header=FALSE,type="latex",summary=FALSE)
```

Asymptotic convergence is slower but bias is reduced eventually. Standard deviations are higher due to higher presence of multicollinearity.

We can also prove that the IV estimate has higher standard deviation than the OLS one.
$$ $$

4)
As the correlation between Z and X decreases, Z becomes takes on more and more of the characteristics of a weak instrument until it becomes an invalid instrument at 0. Bias increases accordingly, and the convergence of the estimator is slower.
```{r}
dataQ4 = regMCloop$new(nrange=200,sigmarange=1,Urange="norm",a1range=c(0.8,seq(0.6,0,by=-0.1)))
dataQ4$loop(b3=1,IV=TRUE)
```

```{r results='asis',echo=FALSE}
    Bias = vector(length=8)
    SD = vector(length=8)
    CovRate = vector(length=8)
    CovLength = vector(length=8)
    for(i in 1:8){
    Bias[i] = (dataQ4$results[[i]]$bias)
    SD[i] =   (dataQ4$results[[i]]$sd)
    CovRate[i] = (dataQ4$results[[i]]$CovRate)
    CovLength[i]=(dataQ4$results[[i]]$CovLength)
    }
    ndf=rep(200,8)
    sigma  = rep(1,8)
    Distribution = rep("Normal",8)
    Q4=data.frame(ndf,sigma,Distribution,Bias,SD,CovRate,CovLength)
    stargazer(Q4,header=FALSE,type="latex",summary=FALSE)
```

5)
```{r}
dataQ5 = regMCloop$new(nrange=200,sigmarange=1,Urange="norm",a2range=100)
dataQ5$loop()
```
```{r}
dataQ5$results[[1]]
```

```{r results='asis',echo=FALSE}
    Bias = vector(length=5)
    SD = vector(length=5)
    CovRate = vector(length=5)
    CovLength = vector(length=5)
    for(i in 1:5){
    Bias[i] = (dataQ5$results[[i]]$bias)
    SD[i] =   (dataQ5$results[[i]]$sd)
    CovRate[i] = (dataQ5$results[[i]]$CovRate)
    CovLength[i]=(dataQ5$results[[i]]$CovLength)
    }
    ndf=rep(200,5)
    sigma  = rep(1,5)
    Distribution = rep("Normal",5)
    Q4=data.frame(ndf,sigma,Distribution,Bias,SD,CovRate,CovLength)
    stargazer(Q4,header=FALSE,type="latex",summary=FALSE)
```
